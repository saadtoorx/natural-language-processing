# LLaMA Text Summarizer

A production-ready text summarization application powered by a local LLaMA model via Ollama. Features a FastAPI backend and a modern Streamlit frontend.

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.127+-green.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.49+-red.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

## âœ¨ Features

- ğŸ“ **Text Summarization** - Summarize any text with AI
- ğŸ“„ **PDF Support** - Upload and summarize PDF documents
- ğŸ”„ **Multiple Modes** - Brief, Detailed, or Bullet Points
- ğŸŒ **Multi-language** - Auto-detects language and responds accordingly
- ğŸŒ™ **Dark Mode** - Toggle dark/light theme
- ğŸ“œ **History** - View your last 5 summaries
- ğŸ’¾ **Download** - Save summaries as TXT files
- ğŸ”’ **Privacy** - All processing happens locally

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     HTTP      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     HTTP      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚   POST/GET    â”‚                 â”‚   /api        â”‚                 â”‚
â”‚   Streamlit     â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚    FastAPI      â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚     Ollama      â”‚
â”‚   Frontend      â”‚               â”‚    Backend      â”‚               â”‚     (LLaMA)     â”‚
â”‚   (Port 8501)   â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚   (Port 8000)   â”‚ â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚   (Port 11434)  â”‚
â”‚                 â”‚     JSON      â”‚                 â”‚     JSON      â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Prerequisites

- **Python 3.10+** - [Download](https://www.python.org/downloads/)
- **Ollama** - [Install Ollama](https://ollama.ai/download)
- **LLaMA 2 Model** - Run `ollama pull llama2` after installing Ollama

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/saadtoorx/llama-text-summarizer.git
cd llama-text-summarizer
```

### 2. Create Virtual Environment

```bash
python -m venv venv

# Windows
.\venv\Scripts\activate

# macOS/Linux
source venv/bin/activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Configure Environment (Optional)

```bash
cp .env.example .env
# Edit .env with your settings
```

### 5. Start Ollama

Make sure Ollama is running:
```bash
ollama serve
```

### 6. Run the Application

**Terminal 1 - Start Backend:**
```bash
uvicorn backend.main:app --reload
```

**Terminal 2 - Start Frontend:**
```bash
streamlit run frontend/app.py
```

### 7. Open the App

Navigate to `http://localhost:8501` in your browser.

## ğŸ“š API Documentation

Once the backend is running, visit:
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/health` | Check API and Ollama status |
| POST | `/summarize/` | Summarize text |
| POST | `/summarize/pdf/` | Summarize PDF file |

## ğŸ¨ Screenshots

*Coming soon*

## ğŸ› ï¸ Configuration

Environment variables (set in `.env`):

| Variable | Default | Description |
|----------|---------|-------------|
| `OLLAMA_URL` | `http://localhost:11434` | Ollama API URL |
| `OLLAMA_MODEL` | `llama2` | Model to use |
| `BACKEND_URL` | `http://localhost:8000` | Backend API URL |
| `MAX_TEXT_LENGTH` | `10000` | Max input characters |
| `MIN_TEXT_LENGTH` | `10` | Min input characters |

## ğŸ“ Project Structure

```
llama-text-summarizer/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ main.py          # FastAPI application
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py           # Streamlit application
â”œâ”€â”€ .env.example         # Environment template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.ai/) - Local LLM runner
- [Meta AI](https://ai.meta.com/) - LLaMA model
- [FastAPI](https://fastapi.tiangolo.com/) - Modern Python web framework
- [Streamlit](https://streamlit.io/) - Data app framework
